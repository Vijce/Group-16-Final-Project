{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install sentence-transformers joblib"
      ],
      "metadata": {
        "id": "2I1UHdnRVaO6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load senoiority model（LLM）"
      ],
      "metadata": {
        "id": "5Aa_sCe02cgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5GBw5agVMqU",
        "outputId": "25666eba-2f33-41d1-da2f-455fc4b5e2c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/final_seniority_distilbert models/"
      ],
      "metadata": {
        "id": "ykV7fg7mxggp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls models/final_seniority_distilbert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg8q1_IzxqNF",
        "outputId": "f26b85a5-0930-4685-a55e-279feb2b7c95"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id2label.json  label2id.json  model  tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load senoiority model\n",
        "import os, json, torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "SEN_DIR = \"models/final_seniority_distilbert\"\n",
        "\n",
        "sen_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    os.path.join(SEN_DIR, \"tokenizer\"),\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "sen_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    os.path.join(SEN_DIR, \"model\"),\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "with open(os.path.join(SEN_DIR, \"id2label.json\")) as f:\n",
        "    sen_id2label = {int(k): v for k, v in json.load(f).items()}\n",
        "\n",
        "sen_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tDI564rBx90B",
        "outputId": "5892a60a-13f6-4fef-db08-2f910cb40b15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load department model（SentenceTransformer + MLP）"
      ],
      "metadata": {
        "id": "_gTOhIK03IMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load department model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import joblib, json\n",
        "import numpy as np\n",
        "\n",
        "MODEL_DIR = \"models/final_department_embed_mlp\"\n",
        "\n",
        "embedder = SentenceTransformer(f\"{MODEL_DIR}/sentence_model\")\n",
        "clf = joblib.load(f\"{MODEL_DIR}/mlp.pkl\")\n",
        "\n",
        "with open(f\"{MODEL_DIR}/classes.json\", \"r\") as f:\n",
        "    classes = json.load(f)\n",
        "\n",
        "print(\" Loaded Department model.\")\n",
        "print(\"Num classes:\", len(classes))\n",
        "print(\"First 5 classes:\", classes[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SIYgcPrVoFl",
        "outputId": "b4d8ce17-1a05-48c1-e799-3dc0787c3cde"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loaded Department model.\n",
            "Num classes: 11\n",
            "First 5 classes: ['Administrative', 'Business Development', 'Consulting', 'Customer Support', 'Human Resources']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute embedding\n",
        "class_emb = embedder.encode(classes, normalize_embeddings=True)\n",
        "\n",
        "def cosine_topk(text, k=5):\n",
        "    q = embedder.encode([text], normalize_embeddings=True)[0]\n",
        "    sims = class_emb @ q\n",
        "    top_idx = np.argsort(-sims)[:k]\n",
        "    return [(classes[i], float(sims[i])) for i in top_idx]\n"
      ],
      "metadata": {
        "id": "cOzEGscvYy72"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explain"
      ],
      "metadata": {
        "id": "NDwsj9G74658"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cosine explain for department\n",
        "def predict_department_with_explain(text, k=5):\n",
        "    emb = embedder.encode([text])\n",
        "    probs = clf.predict_proba(emb)[0]\n",
        "    pred_idx = int(np.argmax(probs))\n",
        "\n",
        "    pred_label = classes[pred_idx]\n",
        "    pred_conf = float(probs[pred_idx])\n",
        "\n",
        "    topk = cosine_topk(text, k=k)\n",
        "    return pred_label, pred_conf, topk\n",
        "\n",
        "# probability explain for seniority\n",
        "def predict_seniority_with_probs(text, topk=3):\n",
        "    \"\"\"\n",
        "    return：\n",
        "    - pred_label (str)\n",
        "    - pred_conf (float)\n",
        "    - df_probs (DataFrame): top-k\n",
        "    \"\"\"\n",
        "    inputs = sen_tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = sen_model(**inputs).logits  # [1, num_labels]\n",
        "        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
        "\n",
        "    # top-k probability\n",
        "    idx = np.argsort(-probs)[:topk]\n",
        "    rows = [(sen_id2label[int(i)], float(probs[i])) for i in idx]\n",
        "    df = pd.DataFrame(rows, columns=[\"Seniority\", \"Probability\"])\n",
        "\n",
        "    pred_label = rows[0][0]\n",
        "    pred_conf  = rows[0][1]\n",
        "    return pred_label, pred_conf, df\n"
      ],
      "metadata": {
        "id": "LKem1YMgV99F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI Design"
      ],
      "metadata": {
        "id": "ZvjQr5Gb3oHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "def ui_predict(text, dept_k, sen_k):\n",
        "    if not text or not text.strip():\n",
        "        return \"Please enter a job title / position text.\", None, \"Please enter a job title / position text.\", None\n",
        "\n",
        "    # A) Department (MLP + cosine explain)\n",
        "    dept_label, dept_conf, dept_topk = predict_department_with_explain(text, k=dept_k)\n",
        "    dept_warn = \" low confidence\" if dept_conf < 0.40 else \"\"\n",
        "    dept_summary = f\"Predicted Department: {dept_label} (conf={dept_conf:.3f}){dept_warn}\"\n",
        "    dept_df = pd.DataFrame(dept_topk, columns=[\"Domain (prototype)\", \"Cosine similarity\"])\n",
        "\n",
        "    # B) Seniority (DistilBERT + prob distribution explain)\n",
        "    sen_label, sen_conf, sen_df = predict_seniority_with_probs(text, topk=sen_k)\n",
        "    sen_warn = \" low confidence\" if sen_conf < 0.40 else \"\"\n",
        "    sen_summary = f\"Predicted Seniority: {sen_label} (conf={sen_conf:.3f}){sen_warn}\"\n",
        "\n",
        "    return dept_summary, dept_df, sen_summary, sen_df\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=ui_predict,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, label=\"LinkedIn position text\"),\n",
        "        gr.Slider(3, 10, value=5, step=1, label=\"Department explainability: Top-K cosine similarities\"),\n",
        "        gr.Slider(2, 5, value=3, step=1, label=\"Seniority explainability: Top-K probabilities\"),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Department prediction\"),\n",
        "        gr.Dataframe(label=\"Explainability (Department): Top-K cosine similarities\", interactive=False),\n",
        "\n",
        "        gr.Textbox(label=\"Seniority prediction\"),\n",
        "        gr.Dataframe(label=\"Explainability (Seniority): Top-K probability distribution\", interactive=False),\n",
        "    ],\n",
        "    title=\"LinkedIn Classifier (Department + Seniority)\",\n",
        "    description=\"Prototype demo for SnapAddy capstone with embedding-based explainability (dept) and probability-based explainability (seniority).\"\n",
        ")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "oxYLyeUYWFW_",
        "outputId": "ba55bb2c-540a-4927-b505-ad4a4bc45c68"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1a80b083ae57162dab.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1a80b083ae57162dab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}